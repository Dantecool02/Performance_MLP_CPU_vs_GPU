{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d898Vmx3hojs"
      },
      "source": [
        "### Optimizations to make:\n",
        "\n",
        "- Make use of torch\n",
        "- Run the torch optimized code on GPUs (Still not decided which GPU)\n",
        "- If we can use torch.compile() somehow, in combination with above\n",
        "- Can we use lower precision on the float values, bfloat16?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk_Q8dafhojt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zEMYKf0hoju",
        "outputId": "45890c5d-36dd-4662-c4c5-3bb85956848f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Adjust file reading if we are on google colab\n",
        "\n",
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    train_path = '/content/drive/MyDrive/DD2358_Data/train.csv'\n",
        "    test_path = '/content/drive/MyDrive/DD2358_Data/test.csv'\n",
        "else:\n",
        "    train_path = '../data/train.csv'\n",
        "    test_path = '../data/test.csv'\n",
        "\n",
        "\n",
        "# Seeds\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzV4jcC1hoju"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbZgPrtihoju",
        "outputId": "a29aee0c-5c39-41cf-a097-a9b12598607d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train file: 2700\n",
            "Number of samples in test file: 675\n",
            "\n",
            "Number of class 0 in train file: 900, 0.33% of train set\n",
            "Number of class 1 in train file: 900, 0.33% of train set\n",
            "Number of class 2 in train file: 900, 0.33% of train set\n",
            "\n",
            "Number of class 0 in test file: 225, 0.33% of test set\n",
            "Number of class 1 in test file: 225, 0.33% of test set\n",
            "Number of class 0 in test file: 225, 0.33% of test set\n"
          ]
        }
      ],
      "source": [
        "# Read files\n",
        "train_df = pd.read_csv(train_path, header = None)\n",
        "test_df = pd.read_csv(test_path, header = None)\n",
        "\n",
        "\n",
        "# Check data sizes and class balances\n",
        "print(\"Number of samples in train file: \" + str(train_df.shape[0]))\n",
        "print(\"Number of samples in test file: \" + str(test_df.shape[0]))\n",
        "\n",
        "# Check Class balance in train and test\n",
        "print(\"\\nNumber of class 0 in train file: \" + str(train_df[0].eq(0).sum()) + f\", {float(train_df[0].eq(0).sum()/train_df.shape[0]):.2f}% of train set\")\n",
        "print(\"Number of class 1 in train file: \" + str(train_df[0].eq(1).sum()) + f\", {float(train_df[0].eq(1).sum()/train_df.shape[0]):.2f}% of train set\")\n",
        "print(\"Number of class 2 in train file: \" + str(train_df[0].eq(2).sum()) + f\", {float(train_df[0].eq(2).sum()/train_df.shape[0]):.2f}% of train set\")\n",
        "\n",
        "print(\"\\nNumber of class 0 in test file: \" + str(test_df[0].eq(0).sum()) + f\", {float(test_df[0].eq(0).sum()/test_df.shape[0]):.2f}% of test set\")\n",
        "print(\"Number of class 1 in test file: \" + str(test_df[0].eq(1).sum()) + f\", {float(test_df[0].eq(1).sum()/test_df.shape[0]):.2f}% of test set\")\n",
        "print(\"Number of class 0 in test file: \" + str(test_df[0].eq(2).sum()) + f\", {float(test_df[0].eq(2).sum()/test_df.shape[0]):.2f}% of test set\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gp-a5gOhoju"
      },
      "source": [
        "### Datasets are perfectly balanced, lets split it into X and Y and normalise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "-luiEbjrhoju",
        "outputId": "784fff94-f031-4707-c1ed-58e5a6d25625"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train data\n",
        "train_X = train_df.iloc[:, 1:].values.astype(np.float32)\n",
        "train_Y = train_df.iloc[:, 0].values\n",
        "\n",
        "# Test data\n",
        "test_X = test_df.iloc[:, 1:].values.astype(np.float32)\n",
        "test_Y = test_df.iloc[:, 0].values\n",
        "\n",
        "# Normalise the X values to [0,1]\n",
        "train_X /= np.float32(255)\n",
        "test_X /= np.float32(255)\n",
        "\n",
        "# Convert to tensors\n",
        "train_X = torch.tensor(train_X)\n",
        "train_Y = torch.tensor(train_Y)\n",
        "test_X = torch.tensor(test_X)\n",
        "test_Y = torch.tensor(test_Y)\n",
        "\n",
        "sample_image = train_X[200,:].reshape(32,32)\n",
        "plt.imshow(sample_image, cmap='gray', vmin=0, vmax=1)\n",
        "plt.title(\"Sample 32x32 Grayscale Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x78_btAHhojv"
      },
      "source": [
        "### Model parameters and constructing dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTpDhArhhojv",
        "outputId": "dd6fe5fb-57ba-40a5-e22c-d87ebd4d44c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2700\n"
          ]
        }
      ],
      "source": [
        "# Dataloader\n",
        "batch_size = train_X.shape[0]\n",
        "print(batch_size)\n",
        "\n",
        "train_dataset = TensorDataset(train_X, train_Y)\n",
        "test_dataset  = TensorDataset(test_X, test_Y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model parameters\n",
        "input_dim = train_X.shape[1]  # 1024 pixels\n",
        "hidden_dim1 = 8\n",
        "#hidden_dim2 = 64\n",
        "output_dim = 3  # 3 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogc4I-52hojv"
      },
      "source": [
        "### Construct the Model with nn Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FTnwRVNhojv",
        "outputId": "eead3067-24c4-4df4-d993-a4d58b73c492"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Key note: ReLU was way better than Sigmoid by default in torch, this is because\n",
        "# Weights initilization is Kaiming uniform, and not Xavi which is needed for Sigmoid\n",
        "\n",
        "# Great model is lr = 0.001, decrease of factor 0.8 with patience = 10 on linear lr_scheduler\n",
        "\n",
        "class MLP (nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim1, bias=True),\n",
        "            #nn.BatchNorm1d(hidden_dim1),\n",
        "            nn.Sigmoid(),\n",
        "            #nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim1, output_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # Initialize weights uniformly in the range [-0.2, 0.2]\n",
        "        nn.init.uniform_(m.weight, -0.2, 0.2)\n",
        "        # Optionally, initialize biases to zero\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "# COMPILATION OPTIMIZATION\n",
        "use_compile = True # Set to False to disable compilation\n",
        "\n",
        "model = MLP(input_dim, hidden_dim1, output_dim).to(device)\n",
        "model.apply(init_weights)\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=10, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js7JVgmohojv"
      },
      "source": [
        "### Lets train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OQ8RFTJ9hojv",
        "outputId": "711127f3-8666-481f-e92e-7e9ac811ae51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "c:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "BackendCompilerFailed",
          "evalue": "backend='inductor' raised:\nRuntimeError: Compiler: cl is not found.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass in Bfloat16, OPTIMIZATION dtype\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Necessary after OPTIMIZATION of dtype = Bfloat16\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass with scaling\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    579\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1380\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1375\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1376\u001b[0m             )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1164\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m   1162\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:547\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    544\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:986\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    984\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 986\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     put_code_state()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:715\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    713\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks())\n\u001b[0;32m    714\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_utils_internal.py:95\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[0;32m     98\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     99\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:750\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    748\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 750\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1361\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1358\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1359\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1361\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:231\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:662\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 662\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    664\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2868\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2868\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:1052\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1052\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1053\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:962\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:3048\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   3047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 3048\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:3033\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   3028\u001b[0m _step_logger()(\n\u001b[0;32m   3029\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   3030\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3031\u001b[0m )\n\u001b[0;32m   3032\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[1;32m-> 3033\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   3037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3039\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   3040\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[0;32m   3043\u001b[0m )\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1101\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m   1098\u001b[0m append_prefix_insts()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m   1105\u001b[0m )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m   1108\u001b[0m     [\n\u001b[0;32m   1109\u001b[0m         PyCodegen(tx, overridden_sources\u001b[38;5;241m=\u001b[39moverridden_sources)\u001b[38;5;241m.\u001b[39mcreate_store(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     ]\n\u001b[0;32m   1114\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1382\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root, replaced_outputs)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1382\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[0;32m   1393\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1432\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1428\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1429\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1430\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1431\u001b[0m     ):\n\u001b[1;32m-> 1432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1483\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m   1484\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m   1485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m signpost_event(\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1495\u001b[0m     },\n\u001b[0;32m   1496\u001b[0m )\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1462\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1461\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1462\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:130\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\__init__.py:2340\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   2338\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 2340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1863\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1859\u001b[0m     tracing_context\n\u001b[0;32m   1860\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39m_disable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[0;32m   1861\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m ):\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py:83\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 83\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1155\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m   1146\u001b[0m         dispatch_and_compile,\n\u001b[0;32m   1147\u001b[0m         mod,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m         remote,\n\u001b[0;32m   1153\u001b[0m     )\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1155\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1131\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1129\u001b[0m functional_call \u001b[38;5;241m=\u001b[39m create_functional_call(mod, params_spec, params_len)\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39m_disable():\n\u001b[1;32m-> 1131\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:580\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[0;32m    573\u001b[0m     flat_fn,\n\u001b[0;32m    574\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[0;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:830\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[0;32m    828\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[1;32m--> 830\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:678\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    675\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m inner_meta\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 678\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    681\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m make_boxed_func(compiled_fw_func)\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:489\u001b[0m, in \u001b[0;36mSerializableAOTDispatchCompiler.__call__\u001b[1;34m(self, gm, example_inputs)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    486\u001b[0m     gm: torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule,\n\u001b[0;32m    487\u001b[0m     example_inputs: Sequence[InputType],\n\u001b[0;32m    488\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutputCode:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1741\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(gm, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1739\u001b[0m     model_outputs_node\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_visible_output_idxs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:569\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[0;32m    564\u001b[0m get_chromium_event_logger()\u001b[38;5;241m.\u001b[39madd_event_data(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    566\u001b[0m     is_backward\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    567\u001b[0m )\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_compiler_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compile_fx_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:102\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:685\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 685\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1129\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfx_codegen_and_compile\u001b[39m(\n\u001b[0;32m   1120\u001b[0m     gm: GraphModule,\n\u001b[0;32m   1121\u001b[0m     example_inputs: Sequence[InputType],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_kwargs: Unpack[_CompileFxKwargs],\n\u001b[0;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutputCode:\n\u001b[0;32m   1127\u001b[0m     scheme: FxCompile \u001b[38;5;241m=\u001b[39m _InProcessFxCompile()\n\u001b[1;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1044\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[1;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[0;32m   1036\u001b[0m             compiled_fn \u001b[38;5;241m=\u001b[39m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1037\u001b[0m                 graph,\n\u001b[0;32m   1038\u001b[0m                 code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m                 additional_files\u001b[38;5;241m=\u001b[39madditional_files,\n\u001b[0;32m   1042\u001b[0m             )\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1044\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m   1046\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n\u001b[0;32m   1047\u001b[0m metrics\u001b[38;5;241m.\u001b[39mnum_bytes_accessed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_bytes\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\graph.py:2027\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModuleType:\n\u001b[0;32m   2021\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   2022\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2023\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2024\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2025\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2026\u001b[0m     ):\n\u001b[1;32m-> 2027\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\graph.py:2033\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModuleType:\n\u001b[0;32m   2030\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   2032\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2034\u001b[0m     )\n\u001b[0;32m   2035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mautotune_at_compile_time:\n\u001b[0;32m   2036\u001b[0m         tuning_code \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2038\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile-time auto-tuning block: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2041\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2042\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\graph.py:1968\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1965\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1971\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1972\u001b[0m     V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mall_codegen_kernel_names,\n\u001b[0;32m   1973\u001b[0m )\n\u001b[0;32m   1975\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_inference)\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:3477\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3476\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.codegen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:3554\u001b[0m, in \u001b[0;36mScheduler._codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3552\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcodegen_combo_kernel(node)\n\u001b[0;32m   3553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[1;32m-> 3554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3556\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4781\u001b[0m, in \u001b[0;36mCppScheduling.codegen_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   4779\u001b[0m nodes: List[SchedulerNode] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtry_loop_split(nodes)\n\u001b[1;32m-> 4781\u001b[0m cpp_kernel_proxy \u001b[38;5;241m=\u001b[39m \u001b[43mCppKernelProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4782\u001b[0m cpp_kernel_proxy\u001b[38;5;241m.\u001b[39mcodegen_nodes(nodes)\n\u001b[0;32m   4783\u001b[0m kernel_group\u001b[38;5;241m.\u001b[39mfinalize_kernel(cpp_kernel_proxy, nodes)\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3632\u001b[0m, in \u001b[0;36mCppKernelProxy.__init__\u001b[1;34m(self, kernel_group)\u001b[0m\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop_nest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_ranges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpicked_vec_isa: cpu_vec_isa\u001b[38;5;241m.\u001b[39mVecISA \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_vec_isa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels: List[CppKernel] \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:414\u001b[0m, in \u001b[0;36mpick_vec_isa\u001b[1;34m()\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform\u001b[38;5;241m.\u001b[39mmachine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx86_64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD64\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[1;32m--> 414\u001b[0m _valid_vec_isa_list: List[VecISA] \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:401\u001b[0m, in \u001b[0;36mvalid_vec_isa_list\u001b[1;34m()\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[1;32m--> 401\u001b[0m     \u001b[43misa_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:401\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[1;32m--> 401\u001b[0m     isa_list\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m    402\u001b[0m         isa\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa)\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:142\u001b[0m, in \u001b[0;36mVecISA.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:152\u001b[0m, in \u001b[0;36mVecISA.__bool__impl\u001b[1;34m(self, vec_isa_ok)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode():\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:102\u001b[0m, in \u001b[0;36mVecISA.check_build\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     CppBuilder,\n\u001b[0;32m     95\u001b[0m     CppTorchOptions,\n\u001b[0;32m     96\u001b[0m     normalize_path_separator,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     99\u001b[0m key, input_path \u001b[38;5;241m=\u001b[39m write(\n\u001b[0;32m    100\u001b[0m     code,\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 102\u001b[0m     extra\u001b[38;5;241m=\u001b[39m\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfilelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[0;32m    106\u001b[0m lock_dir \u001b[38;5;241m=\u001b[39m get_lock_dir()\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:28\u001b[0m, in \u001b[0;36m_get_isa_dry_compile_fingerprint\u001b[1;34m(isa_flags)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[1;32m---> 28\u001b[0m     compiler_info \u001b[38;5;241m=\u001b[39m get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m     30\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:144\u001b[0m, in \u001b[0;36mget_cpp_compiler\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_WINDOWS:\n\u001b[0;32m    143\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCXX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode():\n",
            "File \u001b[1;32mc:\\Users\\dante\\skola\\DD2358-Introduction-to-High-Performance-Computing\\env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:135\u001b[0m, in \u001b[0;36mcheck_compiler_exist_windows\u001b[1;34m(compiler)\u001b[0m\n\u001b[0;32m    129\u001b[0m     output_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    130\u001b[0m         subprocess\u001b[38;5;241m.\u001b[39mcheck_output([compiler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/help\u001b[39m\u001b[38;5;124m\"\u001b[39m], stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT)\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m*\u001b[39mSUBPROCESS_DECODE_ARGS)\n\u001b[0;32m    133\u001b[0m     )\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mSubprocessError:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Compiler: cl is not found.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 600\n",
        "patience = 100  # if no improvement for x epochs, stop training\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improve = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "best_model_state = None\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    scaler = torch.amp.GradScaler(\"cuda\")\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass in Bfloat16, OPTIMIZATION dtype\n",
        "        with torch.amp.autocast(\"cuda\",dtype=torch.bfloat16):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Necessary after OPTIMIZATION of dtype = Bfloat16\n",
        "        # Backward pass with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate on validation (test) set\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Change for dtype = Bfloat16 OPTIMIZATION\n",
        "            with torch.amp.autocast(\"cuda\",dtype=torch.bfloat16):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(test_dataset)\n",
        "    val_acc = 100.0 * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improve = 0\n",
        "    else:\n",
        "        epochs_without_improve += 1\n",
        "        if epochs_without_improve >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Plot Training and Validation Curves\n",
        "# =============================================================================\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Curves\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\", color=\"green\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# Final Evaluation on Test Set\n",
        "# =============================================================================\n",
        "\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "final_acc = 100.0 * correct / total\n",
        "print(\"Final Test Accuracy: {:.2f}%\".format(final_acc))\n",
        "\n",
        "# End timer and print execution time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Total Execution Time: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVrtf-kdhojv"
      },
      "source": [
        "### General Notes from testing:\n",
        "\n",
        "Double hidden layer with more neurons in first than second improvs the models accuracy potential. Using Relu, dropout(0.3) and layersizes = [input_dim, 128, 64, output_dim] worked well with this. Good generelization. Really seems to be aproblem in need of two hidden layers\n",
        "\n",
        "However, Dropout is only relevant if we have large enough network, otherwise the model becomes way too simple and cant learn properly\n",
        "\n",
        "For Double layer architecure the nn.BatchNordm1d(any_hidden_layer_dim) after the nn.linear and before activation function is absolutely fundamental. Without - > 46% final accuracy, with it for both the linear from input to hidden1 and from hidden1 to hidden2 we get 86% accuracy. Absolutely fundamental.\n",
        "\n",
        "Significant improvement for this dataset with ReLu in comparisson to Sigmoid, 70 -> 86% validation accuracy\n",
        "\n",
        "Suspect that the training and test data have bad seed and overall arent a good split, as training probably dont represent test good enough\n",
        "\n",
        "Lower learning rate = More stable improvement, but does converge earlier with a little worse accuracy\n",
        "\n",
        "\n",
        "\n",
        "TODO:\n",
        "\n",
        "To pass assignment:\n",
        "- Fix the loss and optimizer to somehow incorporate the customized loss and optimizer in the original code. The problem is getting that to run with torch\n",
        "\n",
        "For fun:\n",
        "- Understand BatchNorm1d\n",
        "- Dropout effect on different layer sizes\n",
        "- Should the layer sizes in a MLP only decrease layer by layer (Not risk the problem of simply copying from layer to another, there was fix for this problem though somehow (See sparse autoencdoers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJKzU74Fhojv"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
